<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Kai Wu | Methods for non-linear least squares problems</title>
  <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <link rel="shortcut icon" href="https://imkaywu.github.io/assets/img/favicon.ico">
  <link rel="stylesheet" href="https://imkaywu.github.io/assets/css/main.css">
  <link rel="canonical" href="https://imkaywu.github.io/blog/2016/05/non-linear-optimization/">

  
</head>

  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML'></script>

  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Kai</strong> Wu
    </span>
    

    <nav class="site-nav">

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://imkaywu.github.io/">about</a>

        <!-- Blog -->
        <a class="page-link" href="https://imkaywu.github.io/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
            <a class="page-link" href="https://imkaywu.github.io/code/">code</a>
          
        
          
        
          
            <a class="page-link" href="https://imkaywu.github.io/links/">links</a>
          
        
          
        
          
            <a class="page-link" href="https://imkaywu.github.io/projects/">projects</a>
          
        
          
            <a class="page-link" href="https://imkaywu.github.io/tutorials/">tutorials</a>
          
        
          
        
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="https://imkaywu.github.io/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Methods for non-linear least squares problems</h1>
    <p class="post-meta">May 4, 2016</p>
  </header>

  <article class="post-content">
    <p>The content in this post is not original, the reference list is at the end of the post.</p>

<h2 id="definition-of-least-squares-problem">Definition of Least Squares Problem</h2>

<div class="notice--primary">
  
<h3 id="definition-11-least-squares-problem">Definition 1.1. Least Squares Problem</h3>

<p>Find \(\mathbf{x}^*\), a local minimizer for</p>

<script type="math/tex; mode=display">F(x)=\frac{1}{2}\sum_{i=1}^m (f_i(\mathbf{x}))^2</script>

<p>where \(f_i: \mathbf{R}^n \mapsto \mathbf{R}, i=1,…,m\) are given functions, and \(m\geq n\)</p>

</div>

<p>The least squares problem is a special variant of the more general problem: Given a function \(F: \mathbf{R}^n \mapsto \mathbf{R}\), find an argument of \(F\) that gives the minimum value of this so-called <em>objective function</em> or <em>cost function</em>.</p>

<div class="notice--primary">
  
<h3 id="definition-12-global-minimizer">Definition 1.2. Global Minimizer</h3>

<p>Given \(F: \mathbf{R}^n \mapsto \mathbf{R}\). Find</p>

<script type="math/tex; mode=display">\mathbf{x}^+ = argmin_\mathbf{x}{F(\mathbf{x})}.</script>

</div>

<p>This problem is very hard to solve in general, and we only present methods for solving the simpler problem of finding a local minimizer for \(F\), an argument vector which gives a minimum value of \(F\) inside a certain region whose size is given by \(\sigma\), where \(\sigma\) is a small, positive number.</p>

<div class="notice--primary">
  
<h3 id="definition-13-local-minimizer">Definition 1.3. Local Minimizer</h3>

<p>Given \(F: \mathbf{R}^n \mapsto \mathbf{R}\). Find</p>

<p>\(F(\mathbf{x}^*) \leq F(\mathbf{x})\)</p>

<p>for \(\|\mathbf{x}-\mathbf{x}^*\| &lt; \delta\)</p>

</div>

<p>We assume that the cost function \(F\) is differentiable and smooth, which makes the following <em>Taylor expansion</em> valid</p>

<p>\[F(\mathbf{x} + \mathbf{h}) = F(\mathbf{x}) + \mathbf{h}^T \mathbf{g} + \frac{1}{2}\mathbf{h}^T \mathbf{H} \mathbf{h} + \mathit{O}(\|\mathbf{h}\|^3)\]</p>

<p>where \(\mathbf{g}\) is the gradient,</p>

<script type="math/tex; mode=display">\mathbf{g} = F'(\mathbf{x}) = \begin{bmatrix}
								\frac{\partial F}{\partial x_1}(\mathbf{x})\\
								\vdots\\
								\frac{\partial F}{\partial x_n}(\mathbf{x})
								\end{bmatrix}</script>

<p>and \(\mathbf{H}\) is the <em>Hessian</em></p>

<script type="math/tex; mode=display">\mathbf{H}=F''(\mathbf{x}) = \begin{bmatrix}
								\frac{\partial^2 F}{\partial x_i \partial x_j}(\mathbf{x})
								\end{bmatrix}</script>

<p>If \(\mathbf{x}^<em>\) is a local minimizer and \(|\mathbf{h}|\) is sufficiently small, then we cannot find a point \(\mathbf{x}^</em> + \mathbf{h}\) with a smaller \(F\)-value. We get</p>

<div class="notice--primary">
  
<h3 id="theorem-14-necessary-condition-for-a-local-minimizer">Theorem 1.4. Necessary condition for a local minimizer</h3>

<p>If \(\mathbf{x}^*\) is a local minimizer, then</p>

<script type="math/tex; mode=display">\mathbf{g}^* = F'(\mathbf{x}^*) = 0</script>

</div>

<p>We use a special name for arguments that satisfy the necessary condition:</p>

<div class="notice--primary">
  
<h3 id="definition-15-stationary-point">Definition 1.5. Stationary point</h3>

<p>If \(\mathbf{g}_s = F’(\mathbf{x}_s)=0\)</p>

<p>then \(\mathbf{x}_s\) is said to be a <em>stationary point</em> for \(F\).</p>

</div>

<p>Thus, a local minimizer is a stationary point, but so is a local maximizer. A stationary point which is neither a local maximizer nor a local minimizer is called a saddle point. In order to determine whether a given stationary point is a local minimizer or not, we need to include the second order term in the Taylor series. Insert \(\mathbf{x}_s\) we see that</p>

<script type="math/tex; mode=display">F(\mathbf{x}_s+\mathbf{h})=F(\mathbf{x}_s)+\frac{1}{2}\mathbf{h}^T \mathbf{H}_s \mathbf{h} + \mathit{O}(\|\mathbf{h}\|^3)</script>

<p>with \(\mathbf{H}_s=F’’(\mathbf{x}_s)\)</p>

<p>Because any Hessian is a symmetric matrix. If we request that \(\mathbf{H}_s\) is <em>positive definite</em>, then its eigenvalues are greater than some number \(\delta &gt; 0\), and</p>

<p>\[\mathbf{h}^T \mathbf{H}_s \mathbf{h} &gt; \delta \|\mathbf{h}\|^2\]</p>

<p>For \(\mathbf{h}\) that sufficiently small, the third term \(\mathit{O}(|\mathbf{h}|^3)\) will be dominated by \(\frac{1}{2}\mathbf{h}^T \mathbf{H}_s \mathbf{h}\). This term is positive, so we get</p>

<div class="notice--primary">
  
<h3 id="theorem-16-sufficient-condition-for-a-local-minimizer">Theorem 1.6. Sufficient condition for a local minimizer</h3>

<p>Assume that \(\mathbf{x}_s\) is the stationary point and that \(F’’(\mathbf{x}_s\) is positive definite. Then \(\mathbf{x}_s\) is a local minimizer.</p>

</div>

<p>If \(\mathbf{H}_s\) is negative definite, then \(\mathbf{x}_s\) is a local maximizer. If \(\mathbf{H}_s\) is <em>indefinite</em> (i.e. it has both positive and negative eigenvalues), then \(\mathbf{x}_s\) is a saddle point.</p>

<h2 id="descent-methods">Descent methods</h2>

<p>All methods for non-linear optimization are iterative: from a starting point \(\mathbf{x}_0\), the method produces a series of vectors \(\mathbf{x}_1, \mathbf{x}_2, …\), which hopefully converges to \(\mathbf{x}^<em>\), a local minimizer for the given function. Most methods have measures which enforce the *descending condition</em></p>

<p>\[F(\mathbf{x}_{k+1}) &lt; F(\mathbf{x}_k)\]</p>

<p>This prevents convergence to a maximizer and also makes it less probable that we converge towards a saddle point. If the given function has several minimizers, the result will depend on the starting point \(\mathbf{x}_0\). We don’t know which of the minimizers that will be found, it is not necessarily the one closest to \(\mathbf{x}_0\).</p>

<p>In many cases, the method produces vectors which converge towards the minimizer in two different stages</p>

<ul>
  <li>When \(\mathbf{x}_0\) is far from the solution, we want the method to produce iterates which move steadily towards \(\mathbf{x}^*\).</li>
</ul>

<h2 id="reference">Reference</h2>

<p>[1] K.Madsen, H.B.Nielsen, O.Tingleff, Methods for Non-linear Least Squares Problems</p>

  </article>

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2018 Kai Wu.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://imkaywu.github.io/assets/js/common.js"></script>





<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://imkaywu.github.io/assets/css/font-awesome.min.css">
<link rel="stylesheet" href="https://imkaywu.github.io/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXX-X', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
